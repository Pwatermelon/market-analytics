from fastapi import FastAPI, HTTPException, Depends, Header
from pydantic import BaseModel
from sqlalchemy import create_engine, Column, Integer, String, DateTime, Text, ForeignKey, Float, func
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from datetime import datetime, timedelta
import os
from typing import Optional, List, Dict
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM
import numpy as np
from pathlib import Path

app = FastAPI(
    title="Analyzer Service",
    redirect_slashes=False  # –û—Ç–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã
)

# –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://postgres:postgres@localhost:5432/market_analytics")
engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# –ü—É—Ç—å –∫ –º–æ–¥–µ–ª—è–º
MODEL_PATH = os.getenv("MODEL_PATH", "/app/models")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è)
SENTIMENT_MODEL_NAME = os.getenv("SENTIMENT_MODEL_NAME", None)  # –ï—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é
SUMMARIZER_MODEL_NAME = os.getenv("SUMMARIZER_MODEL_NAME", None)  # –ï—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–µ–π
sentiment_model = None
sentiment_tokenizer = None
summarizer_model = None
summarizer_tokenizer = None


# –ú–æ–¥–µ–ª–∏ –ë–î (–∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏–∑ parser-service —Å—Ç—Ä—É–∫—Ç—É—Ä—É)
class Review(Base):
    __tablename__ = "reviews"
    
    id = Column(Integer, primary_key=True, index=True)
    product_id = Column(Integer, ForeignKey("products.id"), nullable=False, index=True)
    author = Column(String)
    rating = Column(Integer)
    text = Column(Text, nullable=False)
    date = Column(DateTime, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # –ü–æ–ª—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
    sentiment = Column(Float)  # -1 (–Ω–µ–≥–∞—Ç–∏–≤) –¥–æ 1 (–ø–æ–∑–∏—Ç–∏–≤)
    sentiment_label = Column(String)  # positive, negative, neutral
    summary = Column(Text)


class Product(Base):
    __tablename__ = "products"
    
    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, nullable=False, index=True)
    name = Column(String, nullable=False)
    url = Column(String, nullable=False)
    marketplace = Column(String, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)


# Pydantic –º–æ–¥–µ–ª–∏
class SentimentAnalysis(BaseModel):
    sentiment: float
    label: str


class AnalyticsResponse(BaseModel):
    product_id: int
    total_reviews: int
    positive_count: int
    negative_count: int
    neutral_count: int
    average_sentiment: float
    timeline: List[Dict]  # [{date, sentiment, count}]


class SummaryResponse(BaseModel):
    product_id: int
    summary: str
    total_reviews: int


# –£—Ç–∏–ª–∏—Ç—ã
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


def get_user_id(x_user_id: Optional[str] = Header(None)) -> int:
    if not x_user_id:
        raise HTTPException(status_code=401, detail="User ID –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
    return int(x_user_id)


def load_models():
    """–ó–∞–≥—Ä—É–∑–∫–∞ ML –º–æ–¥–µ–ª–µ–π
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–ø–∫–∏ models/
    - –ú–æ–¥–µ–ª–∏ –∏–∑ Hugging Face –ø–æ –∏–º–µ–Ω–∏
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback –Ω–∞ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏
    """
    global sentiment_model, sentiment_tokenizer, summarizer_model, summarizer_tokenizer
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
    print("\n1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏...")
    sentiment_path = Path(MODEL_PATH) / "sentiment"
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –∏–∑ –ø–∞–ø–∫–∏
    if sentiment_path.exists() and any(sentiment_path.iterdir()):
        try:
            print(f"   üìÅ –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞: {sentiment_path}")
            sentiment_tokenizer = AutoTokenizer.from_pretrained(str(sentiment_path))
            sentiment_model = AutoModelForSequenceClassification.from_pretrained(str(sentiment_path))
            sentiment_model.eval()
            print(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {sentiment_path}")
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            print("   üîÑ –ü—Ä–æ–±—É—é –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ Hugging Face...")
            load_sentiment_from_hf()
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: –ú–æ–¥–µ–ª—å –∏–∑ Hugging Face –ø–æ –∏–º–µ–Ω–∏ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞)
    elif SENTIMENT_MODEL_NAME:
        try:
            print(f"   üåê –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ Hugging Face: {SENTIMENT_MODEL_NAME}")
            sentiment_tokenizer = AutoTokenizer.from_pretrained(SENTIMENT_MODEL_NAME)
            sentiment_model = AutoModelForSequenceClassification.from_pretrained(SENTIMENT_MODEL_NAME)
            sentiment_model.eval()
            print(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ Hugging Face")
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ Hugging Face: {e}")
            load_sentiment_from_hf()
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: –î–µ—Ñ–æ–ª—Ç–Ω–∞—è –º–æ–¥–µ–ª—å
    else:
        load_sentiment_from_hf()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
    print("\n2Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏...")
    summarizer_path = Path(MODEL_PATH) / "summarizer"
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –∏–∑ –ø–∞–ø–∫–∏
    if summarizer_path.exists() and any(summarizer_path.iterdir()):
        try:
            print(f"   üìÅ –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞: {summarizer_path}")
            summarizer_tokenizer = AutoTokenizer.from_pretrained(str(summarizer_path))
            summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(str(summarizer_path))
            summarizer_model.eval()
            print(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {summarizer_path}")
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            print("   üîÑ –ü—Ä–æ–±—É—é –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ Hugging Face...")
            load_summarizer_from_hf()
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: –ú–æ–¥–µ–ª—å –∏–∑ Hugging Face –ø–æ –∏–º–µ–Ω–∏ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞)
    elif SUMMARIZER_MODEL_NAME:
        try:
            print(f"   üåê –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ Hugging Face: {SUMMARIZER_MODEL_NAME}")
            summarizer_tokenizer = AutoTokenizer.from_pretrained(SUMMARIZER_MODEL_NAME)
            summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(SUMMARIZER_MODEL_NAME)
            summarizer_model.eval()
            print(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ Hugging Face")
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ Hugging Face: {e}")
            load_summarizer_from_hf()
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: –î–µ—Ñ–æ–ª—Ç–Ω–∞—è –º–æ–¥–µ–ª—å
    else:
        load_summarizer_from_hf()
    
    print("\n" + "=" * 60)
    print("‚úÖ –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ï–ô –ó–ê–í–ï–†–®–ï–ù–ê")
    print("=" * 60)


def load_sentiment_from_hf():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–µ—Ñ–æ–ª—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ Hugging Face"""
    global sentiment_model, sentiment_tokenizer
    try:
        print("   üåê –ó–∞–≥—Ä—É–∑–∫–∞ –¥–µ—Ñ–æ–ª—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏...")
        sentiment_tokenizer = AutoTokenizer.from_pretrained("blanchefort/rubert-base-cased-sentiment")
        sentiment_model = AutoModelForSequenceClassification.from_pretrained("blanchefort/rubert-base-cased-sentiment")
        sentiment_model.eval()
        print("   ‚úÖ –î–µ—Ñ–æ–ª—Ç–Ω–∞—è –º–æ–¥–µ–ª—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        print(f"   ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
        raise


def load_summarizer_from_hf():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–µ—Ñ–æ–ª—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–∑ Hugging Face"""
    global summarizer_model, summarizer_tokenizer
    
    # –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–ø—ã—Ç–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞)
    summarizer_models = [
        "IlyaGusev/rut5_base_sum_gazeta",
        "IlyaGusev/rut5_base_sum_gazeta_v2",
        "cointegrated/rut5-base",
    ]
    
    last_error = None
    for model_name in summarizer_models:
        try:
            print(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {model_name}")
            summarizer_tokenizer = AutoTokenizer.from_pretrained(model_name)
            print(f"‚úì –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –¥–ª—è {model_name}")
            
            summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
            summarizer_model.eval()
            print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_name}")
            return
        except Exception as e:
            last_error = e
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {model_name}: {e}")
            import traceback
            print(traceback.format_exc())
            if model_name != summarizer_models[-1]:
                print("üîÑ –ü—Ä–æ–±—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É...")
                continue
    
    # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ —É–¥–∞–ª–∏—Å—å
    error_msg = f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_error}"
    print(f"‚ùå {error_msg}")
    raise Exception(error_msg)


def analyze_sentiment(text: str) -> SentimentAnalysis:
    """–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
    if sentiment_model is None or sentiment_tokenizer is None:
        raise HTTPException(status_code=500, detail="–ú–æ–¥–µ–ª—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    inputs = sentiment_tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    with torch.no_grad():
        outputs = sentiment_model(**inputs)
        logits = outputs.logits
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    probs = torch.nn.functional.softmax(logits, dim=-1)
    
    # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π
    # –ö–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤—ã–≤–æ–¥–∞
    num_classes = probs.shape[1]
    
    if num_classes == 3:
        # –¢—Ä–µ—Ö–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: negative, neutral, positive
        # –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è: blanchefort/rubert-base-cased-sentiment, –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ä—É—Å—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π
        negative_prob = probs[0][0].item()
        neutral_prob = probs[0][1].item()
        positive_prob = probs[0][2].item()
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —à–∫–∞–ª—É –æ—Ç -1 –¥–æ 1
        sentiment_score = positive_prob - negative_prob
        label_idx = torch.argmax(probs, dim=-1).item()
        labels = ["negative", "neutral", "positive"]
        label = labels[label_idx]
    elif num_classes == 2:
        # –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: negative, positive
        # –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è: –º–Ω–æ–≥–∏–µ –±–∏–Ω–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        sentiment_score = probs[0][1].item() - probs[0][0].item()  # positive - negative
        label = "positive" if sentiment_score > 0 else "negative"
    elif num_classes == 5:
        # –ü—è—Ç–∏–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ—á–µ–Ω—å –Ω–µ–≥–∞—Ç–∏–≤, –Ω–µ–≥–∞—Ç–∏–≤, –Ω–µ–π—Ç—Ä–∞–ª, –ø–æ–∑–∏—Ç–∏–≤, –æ—á–µ–Ω—å –ø–æ–∑–∏—Ç–∏–≤)
        # –ë–µ—Ä–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        if probs.shape[1] == 5:
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –ø–æ—Ä—è–¥–æ–∫: –æ—á–µ–Ω—å –Ω–µ–≥–∞—Ç–∏–≤, –Ω–µ–≥–∞—Ç–∏–≤, –Ω–µ–π—Ç—Ä–∞–ª, –ø–æ–∑–∏—Ç–∏–≤, –æ—á–µ–Ω—å –ø–æ–∑–∏—Ç–∏–≤
            very_negative = probs[0][0].item()
            negative = probs[0][1].item()
            neutral = probs[0][2].item()
            positive = probs[0][3].item()
            very_positive = probs[0][4].item()
            
            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
            sentiment_score = (very_positive + positive * 0.5) - (very_negative + negative * 0.5)
            label_idx = torch.argmax(probs, dim=-1).item()
            labels = ["very_negative", "negative", "neutral", "positive", "very_positive"]
            label = labels[label_idx]
        else:
            # –û–±—â–∏–π —Å–ª—É—á–∞–π –¥–ª—è 5 –∫–ª–∞—Å—Å–æ–≤
            sentiment_score = float(logits[0][-1].item() - logits[0][0].item())
            sentiment_score = np.tanh(sentiment_score)
            label_idx = torch.argmax(probs, dim=-1).item()
            label = f"class_{label_idx}"
    else:
        # –î–ª—è –ª—é–±—ã—Ö –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π (1 –∫–ª–∞—Å—Å, —Ä–µ–≥—Ä–µ—Å—Å–∏—è, –∏ —Ç.–¥.)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π –≤—ã—Ö–æ–¥ –∫–∞–∫ –∑–Ω–∞—á–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        if num_classes == 1:
            # –†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å - –≤—ã—Ö–æ–¥ –Ω–∞–ø—Ä—è–º—É—é
            sentiment_score = float(logits[0][0].item())
            sentiment_score = np.tanh(sentiment_score)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ [-1, 1]
        else:
            # –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –º–æ–¥–µ–ª—å - –±–µ—Ä–µ–º —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –ø–æ—Å–ª–µ–¥–Ω–∏–º –∏ –ø–µ—Ä–≤—ã–º –∫–ª–∞—Å—Å–æ–º
            sentiment_score = float(logits[0][-1].item() - logits[0][0].item())
            sentiment_score = np.tanh(sentiment_score)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–Ω–∞—á–µ–Ω–∏—è
        if sentiment_score > 0.1:
            label = "positive"
        elif sentiment_score < -0.1:
            label = "negative"
        else:
            label = "neutral"
    
    return SentimentAnalysis(sentiment=sentiment_score, label=label)


def summarize_text(text: str, max_length: int = 150) -> str:
    """–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
    if summarizer_model is None or summarizer_tokenizer is None:
        raise Exception("–ú–æ–¥–µ–ª—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    try:
        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        text = text.strip()
        if not text:
            return "–¢–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –ø—É—Å—Ç"
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã
        inputs = summarizer_tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=512,
            padding=True
        )
        
        print(f"üî§ –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {inputs['input_ids'].shape[1]} —Ç–æ–∫–µ–Ω–æ–≤")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        with torch.no_grad():
            try:
                outputs = summarizer_model.generate(
                    **inputs,
                    max_length=max_length,
                    min_length=20,
                    num_beams=4,
                    length_penalty=2.0,
                    early_stopping=True,
                    no_repeat_ngram_size=3,
                    do_sample=False
                )
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –ø—Ä–æ–±—É—é —É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {e}")
                # –ü—Ä–æ–±—É–µ–º —Å —É–ø—Ä–æ—â–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                outputs = summarizer_model.generate(
                    **inputs,
                    max_length=max_length,
                    num_beams=2,
                    early_stopping=True
                )
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        summary = summarizer_tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # –û—á–∏—Å—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        summary = summary.strip()
        
        if not summary:
            return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é"
        
        return summary
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ summarize_text: {e}")
        import traceback
        print(traceback.format_exc())
        raise Exception(f"–û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {str(e)}")


@app.on_event("startup")
async def startup_event():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –∏ –º–∏–≥—Ä–∞—Ü–∏—è –ë–î –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ"""
    # –ú–∏–≥—Ä–∞—Ü–∏—è: –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    from sqlalchemy import text
    with engine.begin() as conn:
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º sentiment
            result = conn.execute(text("""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name='reviews' AND column_name='sentiment'
            """))
            if not result.fetchone():
                print("üîÑ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ sentiment...")
                conn.execute(text("ALTER TABLE reviews ADD COLUMN sentiment FLOAT"))
                print("‚úì –ö–æ–ª–æ–Ω–∫–∞ sentiment –¥–æ–±–∞–≤–ª–µ–Ω–∞")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º sentiment_label
            result = conn.execute(text("""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name='reviews' AND column_name='sentiment_label'
            """))
            if not result.fetchone():
                print("üîÑ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ sentiment_label...")
                conn.execute(text("ALTER TABLE reviews ADD COLUMN sentiment_label VARCHAR"))
                print("‚úì –ö–æ–ª–æ–Ω–∫–∞ sentiment_label –¥–æ–±–∞–≤–ª–µ–Ω–∞")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º summary
            result = conn.execute(text("""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name='reviews' AND column_name='summary'
            """))
            if not result.fetchone():
                print("üîÑ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ summary...")
                conn.execute(text("ALTER TABLE reviews ADD COLUMN summary TEXT"))
                print("‚úì –ö–æ–ª–æ–Ω–∫–∞ summary –¥–æ–±–∞–≤–ª–µ–Ω–∞")
        except Exception as e:
            print(f"‚ö†Ô∏è –ú–∏–≥—Ä–∞—Ü–∏—è –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ (–≤–æ–∑–º–æ–∂–Ω–æ –∫–æ–ª–æ–Ω–∫–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç): {e}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
    try:
        print("üöÄ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π...")
        load_models()
        print("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π: {e}")
        import traceback
        print(traceback.format_exc())
        # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –∑–∞–ø—É—Å–∫, –Ω–æ –º–æ–¥–µ–ª–∏ –Ω–µ –±—É–¥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å
        print("‚ö†Ô∏è –°–µ—Ä–≤–∏—Å –∑–∞–ø—É—â–µ–Ω –±–µ–∑ –º–æ–¥–µ–ª–µ–π. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")


@app.get("/health")
async def health():
    return {
        "status": "ok",
        "sentiment_model_loaded": sentiment_model is not None,
        "summarizer_model_loaded": summarizer_model is not None
    }


@app.post("/analytics/products/{product_id}/analyze")
async def analyze_product_reviews(
    product_id: int,
    db: Session = Depends(get_db),
    user_id: int = Depends(get_user_id)
):
    """–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤ —Ç–æ–≤–∞—Ä–∞"""
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞
    product = db.query(Product).filter(
        Product.id == product_id,
        Product.user_id == user_id
    ).first()
    
    if not product:
        raise HTTPException(status_code=404, detail="–¢–æ–≤–∞—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤ –±–µ–∑ –∞–Ω–∞–ª–∏–∑–∞
    reviews = db.query(Review).filter(
        Review.product_id == product_id,
        Review.sentiment.is_(None)
    ).all()
    
    analyzed_count = 0
    for review in reviews:
        try:
            # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            sentiment_result = analyze_sentiment(review.text)
            review.sentiment = sentiment_result.sentiment
            review.sentiment_label = sentiment_result.label
            
            analyzed_count += 1
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç–∑—ã–≤–∞ {review.id}: {e}")
    
    db.commit()
    
    return {
        "message": "–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω",
        "analyzed_count": analyzed_count
    }


@app.get("/analytics/products/{product_id}", response_model=AnalyticsResponse)
async def get_product_analytics(
    product_id: int,
    start_date: Optional[datetime] = None,
    end_date: Optional[datetime] = None,
    db: Session = Depends(get_db),
    user_id: int = Depends(get_user_id)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø–æ —Ç–æ–≤–∞—Ä—É"""
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞
    product = db.query(Product).filter(
        Product.id == product_id,
        Product.user_id == user_id
    ).first()
    
    if not product:
        raise HTTPException(status_code=404, detail="–¢–æ–≤–∞—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    # –§–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–∞–º
    query = db.query(Review).filter(Review.product_id == product_id)
    if start_date:
        query = query.filter(Review.date >= start_date)
    if end_date:
        query = query.filter(Review.date <= end_date)
    
    reviews = query.filter(Review.sentiment.isnot(None)).all()
    
    if not reviews:
        return AnalyticsResponse(
            product_id=product_id,
            total_reviews=0,
            positive_count=0,
            negative_count=0,
            neutral_count=0,
            average_sentiment=0.0,
            timeline=[]
        )
    
    # –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    total = len(reviews)
    positive_count = sum(1 for r in reviews if r.sentiment_label == "positive")
    negative_count = sum(1 for r in reviews if r.sentiment_label == "negative")
    neutral_count = sum(1 for r in reviews if r.sentiment_label == "neutral")
    average_sentiment = sum(r.sentiment for r in reviews) / total if total > 0 else 0.0
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–∞–º –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–∏–∏
    timeline_data = {}
    for review in reviews:
        date_key = review.date.date()
        if date_key not in timeline_data:
            timeline_data[date_key] = {"sentiments": [], "count": 0}
        timeline_data[date_key]["sentiments"].append(review.sentiment)
        timeline_data[date_key]["count"] += 1
    
    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–∏–∏
    timeline = []
    for date, data in sorted(timeline_data.items()):
        avg_sentiment = sum(data["sentiments"]) / len(data["sentiments"])
        timeline.append({
            "date": date.isoformat(),
            "sentiment": round(avg_sentiment, 3),
            "count": data["count"]
        })
    
    return AnalyticsResponse(
        product_id=product_id,
        total_reviews=total,
        positive_count=positive_count,
        negative_count=negative_count,
        neutral_count=neutral_count,
        average_sentiment=round(average_sentiment, 3),
        timeline=timeline
    )


@app.get("/analytics/products/{product_id}/summary", response_model=SummaryResponse)
async def get_product_summary(
    product_id: int,
    db: Session = Depends(get_db),
    user_id: int = Depends(get_user_id)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤ —Ç–æ–≤–∞—Ä–∞"""
    import traceback
    
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞
        product = db.query(Product).filter(
            Product.id == product_id,
            Product.user_id == user_id
        ).first()
        
        if not product:
            raise HTTPException(status_code=404, detail="–¢–æ–≤–∞—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤
        reviews = db.query(Review).filter(Review.product_id == product_id).all()
        
        if not reviews:
            raise HTTPException(status_code=404, detail="–û—Ç–∑—ã–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        
        print(f"üìä –ü–æ–ª—É—á–µ–Ω–æ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        if summarizer_model is None or summarizer_tokenizer is None:
            error_msg = "–ú–æ–¥–µ–ª—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
            print(f"‚ùå {error_msg}")
            raise HTTPException(status_code=500, detail=error_msg)
        
        # –ü–æ–ª—É—á–∞–µ–º –í–°–ï —Ç–µ–∫—Å—Ç—ã –æ—Ç–∑—ã–≤–æ–≤
        review_texts = [r.text.strip() for r in reviews if r.text and r.text.strip()]
        
        if not review_texts:
            raise HTTPException(status_code=404, detail="–ù–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏")
        
        print(f"üìù –í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç–∑—ã–≤–æ–≤: {len(review_texts)}")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã, —Ä–∞–∑–¥–µ–ª—è—è —Ç–æ—á–∫–∞–º–∏
        all_texts = ". ".join(review_texts)
        total_length = len(all_texts)
        print(f"üìè –û–±—â–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {total_length} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π, —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –∏ —Å—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é
        try:
            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –æ–¥–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (–ø—Ä–∏–º–µ—Ä–Ω–æ 3000 —Ç–æ–∫–µ–Ω–æ–≤ = ~2000 —Å–∏–º–≤–æ–ª–æ–≤)
            MAX_CHUNK_LENGTH = 2000
            
            if total_length <= MAX_CHUNK_LENGTH:
                # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å - —Å—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ–º —Ü–µ–ª–∏–∫–æ–º
                print(f"üìù –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Ü–µ–ª–∏–∫–æ–º...")
                summary = summarize_text(all_texts, max_length=250)
            else:
                # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–Ω—ã–π - —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏
                print(f"üìù –¢–µ–∫—Å—Ç –¥–ª–∏–Ω–Ω—ã–π, —Ä–∞–∑–±–∏–≤–∞—é –Ω–∞ —á–∞—Å—Ç–∏...")
                chunks = []
                current_chunk = ""
                
                for text in review_texts:
                    # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç –ª–∏–º–∏—Ç
                    if len(current_chunk) + len(text) + 2 <= MAX_CHUNK_LENGTH:
                        if current_chunk:
                            current_chunk += ". " + text
                        else:
                            current_chunk = text
                    else:
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫ –∏ –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π
                        if current_chunk:
                            chunks.append(current_chunk)
                        current_chunk = text
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
                if current_chunk:
                    chunks.append(current_chunk)
                
                print(f"üì¶ –†–∞–∑–±–∏—Ç–æ –Ω–∞ {len(chunks)} —á–∞—Å—Ç–µ–π")
                
                # –°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å
                chunk_summaries = []
                for i, chunk in enumerate(chunks, 1):
                    print(f"   üìù –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —á–∞—Å—Ç–∏ {i}/{len(chunks)} ({len(chunk)} —Å–∏–º–≤–æ–ª–æ–≤)...")
                    chunk_summary = summarize_text(chunk, max_length=150)
                    chunk_summaries.append(chunk_summary)
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —á–∞—Å—Ç–µ–π
                combined_summaries = ". ".join(chunk_summaries)
                print(f"üìù –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö —á–∞—Å—Ç–µ–π ({len(combined_summaries)} —Å–∏–º–≤–æ–ª–æ–≤)...")
                
                # –ï—Å–ª–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –≤—Å–µ –µ—â–µ –¥–ª–∏–Ω–Ω—ã–µ, —Å—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ–º –∏—Ö –µ—â–µ —Ä–∞–∑
                if len(combined_summaries) > MAX_CHUNK_LENGTH:
                    summary = summarize_text(combined_summaries, max_length=250)
                else:
                    summary = combined_summaries
            
            print(f"‚úÖ –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –¥–ª–∏–Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {len(summary)} —Å–∏–º–≤–æ–ª–æ–≤")
        except Exception as e:
            error_details = traceback.format_exc()
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            print(f"–î–µ—Ç–∞–ª–∏: {error_details}")
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {str(e)}")
        
        if not summary or len(summary.strip()) == 0:
            summary = "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        
        return SummaryResponse(
            product_id=product_id,
            summary=summary,
            total_reviews=len(reviews)
        )
    except HTTPException:
        raise
    except Exception as e:
        error_details = traceback.format_exc()
        print(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ get_product_summary: {e}")
        print(f"–î–µ—Ç–∞–ª–∏: {error_details}")
        raise HTTPException(status_code=500, detail=f"–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8003)

